---
layout: post
title:  CS224n-Assignment4
date:   2020-08-16 11:04:11 +0300
description: write some words # Add post description (optional)
img: snow_village.jpg # Add image post (optional)
tags: [NLP-Basic]
author: Ricard Huo # Add name author (optional)
comments: true
---
## 奇怪的问题
1. python<3.7的话，会遇到一个奇怪的符号导致后面的注释出现问题，把这个->删了就行，它的功能类似于argparse。
2. LSTM的输入进行了打包处理，操作不当会遇到奇怪的错误，一定要仔细看注释的说明。
3. decode层在`init()`中的定义位置不当，会导致sanity测试出错`python sanity_check.py 1f`，虽然forward层是一样的。
4. 如果想download代码进行测试，记得下载nltk数据库，这边需要一些微操把数据放在该放的地方。

## NMT
1. [(a)->(f)](https://github.com/yingtaoHuo/CS224n-assignment/tree/master/a4)
2. (g)attention经过mask后， (src_len,max_len)之间的0变为-inf， 因为后面有softmax，exp(-无穷)=0，解决exp(0)=1在softmax层占比过大的问题。
3. (j)对于三个attention现在并不是很懂，以后再来填坑，code里用multiplicative attention而不是dot product attention,我一直以为是因为s，h维度不匹配，通过w改变h的维度以便进行bmm操作。以目前所知，dot product attention优势在于计算简单，缺点在于query的size要和h_enc的size必须匹配。additive attention的优势在于可训练f(h_enc,query)可以更好地表达两者间的关系(intution), 缺点在于参数过多(奥卡姆剃刀发出警告)

## Analyzing NMT Systems
1. 不会西班牙语，等以后有机会再填坑。
2. 不想浪费计算资源整这些有的没的，毕竟前人已经证明这套方法的行之有效。

## 模型的训练，使用
### 模型训练
1. batch_iter里的一个小知识点：yield
2. source，target中的句子添加pad，确保文本对齐
3. 定义nn.Embeddings时注意padding_idx的设置
4. BiLSTM对输入语句进行整体编码，获得dec_init（h_projection(h_o)，c_projection(c_o)），h_enc
5. 生成enc_masks,具体目的是标记每个句子结束的部分，计算attention时不会考虑到后面无意义部分，参见NMT_3
6. decode部分，就是把与encode输入句子parallel的句子，逐词输入，每个词各种计算得到一个向量(1*h, 因为一个batch一起训练，所以输出是b/**h)，最后再append，stack到一起
7. 对于每个词，由(1,h)->(1,tgt_len)（经由target_vocab_projection），P大小为(b*tgt_len)
8. 这里的gather函数，类似于一个挑选的意思，这个需要理解一下。gather((tgt_len,b,h),(tgt_len,b,1),dim=-1),返回的是(tgt_len,b,1),这个操作的目的时，从hidden输出中挑选出有意义的那一位（根据单词的one-hot向量）
9. 再根据mask去掉end后面的数值，求和，返回这个batch的总分数
### 使用过程
[2.b](http://web.stanford.edu/class/cs224n/assignments/a4.pdf)