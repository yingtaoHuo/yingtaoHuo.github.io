---
layout: post
title:  CS224n-Assignment4
date:   2020-08-16 11:04:11 +0300
description: write some words # Add post description (optional)
img: snow_village.jpg # Add image post (optional)
tags: [NLP-Basic]
author: Ricard Huo # Add name author (optional)
comments: true
---
## 奇怪的问题
1. python<3.7的话，会遇到一个奇怪的符号导致后面的注释出现问题，把这个->删了就行，它的功能类似于argparse。
2. LSTM的输入进行了打包处理，操作不当会遇到奇怪的错误，一定要仔细看注释的说明。
3. decode层在init()中的定义位置不当，会导致sanity测试出错python sanity_check.py 1f，虽然forward层是一样的。
4. 如果想download代码进行测试，记得下载nltk数据库，这边需要一些微操把数据放在该放的地方。

## NMT
1. [(a)->(f)](https://github.com/yingtaoHuo/CS224n-assignment/tree/master/a4)
2. (g)attention经过mask后， (src_len,max_len)之间的0变为-inf， 因为后面有softmax，exp(-无穷)=0，解决exp(0)=1在softmax层占比过大的问题。
3. (j)对于三个attention现在并不是很懂，以后再来填坑，code里用multiplicative attention而不是dot product attention我一直以为是因为s，h维度不匹配，通过w改变h的维度以便进行bmm操作。以目前所知，dot product attention优势在于计算简单，缺点在于query的size要和h_enc的size必须匹配。additive attention的优势在于可训练f(h_enc,query)可以更好地表达两者间的关系(intution), 缺点在于参数过多(奥卡姆剃刀发出警告)

## Analyzing NMT Systems
1. 不会西班牙语，等以后有机会再填坑。
2. 不想浪费计算资源整这些有的没的，毕竟前人已经证明这套方法的行之有效。