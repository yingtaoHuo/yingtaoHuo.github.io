---
layout: post
title:  CS224n-Assignment3
date:   2020-08-12 21:14:20 +0300
description:  # Add post description (optional)
img: all_covered_snow.jpg # Add image post (optional)
tags: [NLP-Basic]
author: Richard Huo # Add name author (optional)
comments: true
---
# assignment3 write Answer
## Machine Learning & Neural Networks
### (a) Adam Optimizer
1. b=0.9决定了，每次参数更新更大的取决于历史遗留梯度，而非本轮计算出的梯度，所以假设这次梯度varing了，乘一个0.1也不会掀起什么大风大浪。m的出现会使每次更新迭代的步长更加稳定，不会忽小忽大，提高了收敛效率。
2. 这个没理解，数学直觉告诉我，第一个式子已经起到了这个式子的作用。后面有机会结和实际项目，再来填坑。
### (b) Regularization technique-Dropout
1. 1式带入2式
2. 训练期间，dropout可以提高模型的泛化能力。但是在测试期间，dropout会提高模型的不确定性，导致测试结果无法体现模型的performance。

## Dependency Parsing
1. [图](https://spellonyou.github.io/2020/06/cs224n-19w-a3/)
2. 2*n，进n次，出n次
3. [见代码](https://github.com/yingtaoHuo/CS224n-assignment/tree/master/a3/student)

参考目录：
1. [参数初始化](https://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization)
2. https://spellonyou.github.io/2020/06/cs224n-19w-a3/