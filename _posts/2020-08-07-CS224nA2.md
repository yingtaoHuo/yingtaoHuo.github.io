---
layout: post
title:  CS224n-Assignment2
date:   2020-08-07 22:42:20 +0300
description: write some words # Add post description (optional)
img: cat_girl.jpg # Add image post (optional)
tags: [NLP-Basic]
author: Ricard Huo # Add name author (optional)
comments: true
---
1. 本来最近在完成cs224n的课程，就记录一下自己认为推理过程较易堵住的地方吧。
2. 加载glovce预训练模型，因为在墙内，所以比较麻烦，先从我提供的百度云盘中下载glove.6B.zip，解压到同一目录下，`gensim.scripts.glove2word2vec`将其转为word2vec格式的模型，再由`load_word2vec_format`函数进行读取。
3. 思考glove模型的bias问题，bias怎么来的，如何解决
4. word2vec的训练迭代过程
5. type创建类与class的区别，`isinstance()`
6. negative sampling的loss,这个直接就是objective function(那个负对数，需要最小化的值)，但是naive-softmax的loss却是交叉熵，但是loss在这个模型的训练过程中并没有什么用。但是你得明白这一点，不然assignment2没法完成。
7. numpy,dot,broadcast,multiply区别
8. word2vec训练源码：[skip-gram模型](https://github.com/yingtaoHuo/CS224n-assignment/blob/master/a2/word2vec.py)


参考文献
1. https://looperxx.github.io/CS224n-2019-Assignment/
2. http://web.stanford.edu/class/cs224n/
3. http://moverzp.com/2019/05/19/CS-224n-2019-Assignment-2-word2vec-Coding-%E2%80%94%E2%80%94Part-1/
4. [numpy广播机制](https://www.runoob.com/numpy/numpy-broadcast.html)