---
layout: post
title:  CS224n-Assignment2
date:   2020-08-07 22:42:20 +0300
description: write some words # Add post description (optional)
img: cat_girl.jpg # Add image post (optional)
tags: [NLP-Basic]
author: Ricard Huo # Add name author (optional)
comments: true
---
1. 本来最近在完成cs224n的课程，就记录一下自己在完成过程中较易堵住的地方吧。
2. 加载glovce预训练模型，因为在墙内，所以比较麻烦，先从我提供的百度云盘中下载glove.6B.zip，解压到同一目录下，`gensim.scripts.glove2word2vec()`将其转为word2vec格式的模型，再由`load_word2vec_format()`函数进行读取。
3. 思考glove模型的bias问题，bias怎么来的，如何解决
4. word2vec的训练迭代过程
5. type创建类与class的区别，`isinstance()`
6. numpy,dot,broadcast,multiply区别
7. word2vec训练源码：[skip-gram模型](https://github.com/yingtaoHuo/CS224n-assignment/blob/master/a2/word2vec.py)

### word2vec优化方法
#### negativeSampling
以skip-gram为例，优化前，v矩阵即边缘词矩阵每次都要完整的参与运算并完整的更新。negative优化后，会先从v矩阵中提取k个负样本，每个词的提取概率为：
$ P\left(w_{i}\right)=\frac{f\left(w_{i}\right)^{3 / 4}}{\sum_{j=0}^{n}\left(f\left(w_{j}\right)^{3 / 4}\right)}$
$ f\left(w_{i}\right)$是该词的词频，频率越高，越容易被选中。3/4是基于经验的超参。
这样的话，每次更新仅需要更新这k个词，计算量大大减小。

#### hierarchicalSoftmax
[这篇文章写得很好，我就不重复造轮子了。](https://www.cnblogs.com/pinard/p/7243513.html)

参考文献
1. https://looperxx.github.io/CS224n-2019-Assignment/
2. http://web.stanford.edu/class/cs224n/
3. http://moverzp.com/2019/05/19/CS-224n-2019-Assignment-2-word2vec-Coding-%E2%80%94%E2%80%94Part-1/
4. [numpy广播机制](https://www.runoob.com/numpy/numpy-broadcast.html)
5. [word2vec源码](https://github.com/tmikolov/word2vec/blob/master/word2vec.c)