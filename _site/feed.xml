<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2020-08-24T11:54:36+08:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">My personal blog - Richard Huo</title><subtitle>在这里写一些文字，和大家分享NLP方向的前沿动态。
</subtitle><author><name>Richard Huo</name></author><entry><title type="html">Acsa</title><link href="http://localhost:4000/ACSA/" rel="alternate" type="text/html" title="Acsa" /><published>2020-08-21T00:00:00+08:00</published><updated>2020-08-21T00:00:00+08:00</updated><id>http://localhost:4000/ACSA</id><content type="html" xml:base="http://localhost:4000/ACSA/">&lt;ol&gt;
  &lt;li&gt;Aspect based Sentiment Analysis:Aspect-based sentiment analysis is a text analysis technique that breaks down text into aspects (attributes or components of a product or service), and then allocates each one a sentiment level (positive, negative or neutral).&lt;/li&gt;
  &lt;li&gt;Example:Great food but the service was dreadful.
So delicious was the noodles but terrible vegetables.&lt;/li&gt;
  &lt;li&gt;syntax information, task-related syntatic structures is the key.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Aspect-Oriented Dependency Tree&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;For input sentences, apply a dependency parser to obtain its dependency tree, rij is the relation from node i to node j.&lt;/li&gt;
  &lt;li&gt;place the target aspect at the root&lt;/li&gt;
  &lt;li&gt;set the nodes with direct connections to the aspect as the children.&lt;/li&gt;
  &lt;li&gt;other relations are retained, instead introduce a virtual relation n:con from the aspect to each corresponding nodes, where n represents the distance between two nodes. If one sentence include multi-aspects, build a unique tree for each aspect&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Relational Graph Attention Network&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;encode the dependency trees with its labeled edges&lt;/li&gt;
  &lt;li&gt;update each node representation by aggregating neighborhood node representations.&lt;/li&gt;
  &lt;li&gt;extend the original GAT with additional relational heads. We use these heads as gates to control influence fluence from neighborhood nodes. First map the dependency relations to vector.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Model Training&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;BiLSTM encode tree nodes as hi, another BiLSTM encode aspect words as , its average hidden state as the initial representation ha0 of this root.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Baseline Methods&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Syntax-aware models&lt;/li&gt;
  &lt;li&gt;Attention-based models&lt;/li&gt;
  &lt;li&gt;other recent methods&lt;/li&gt;
&lt;/ol&gt;</content><author><name>Richard Huo</name></author><summary type="html">Aspect based Sentiment Analysis:Aspect-based sentiment analysis is a text analysis technique that breaks down text into aspects (attributes or components of a product or service), and then allocates each one a sentiment level (positive, negative or neutral). Example:Great food but the service was dreadful. So delicious was the noodles but terrible vegetables. syntax information, task-related syntatic structures is the key.</summary></entry><entry><title type="html">Lstm</title><link href="http://localhost:4000/LSTM/" rel="alternate" type="text/html" title="Lstm" /><published>2020-08-20T00:00:00+08:00</published><updated>2020-08-20T00:00:00+08:00</updated><id>http://localhost:4000/LSTM</id><content type="html" xml:base="http://localhost:4000/LSTM/"></content><author><name>Richard Huo</name></author><summary type="html"></summary></entry><entry><title type="html">Dataset</title><link href="http://localhost:4000/Dataset/" rel="alternate" type="text/html" title="Dataset" /><published>2020-08-18T16:04:11+08:00</published><updated>2020-08-18T16:04:11+08:00</updated><id>http://localhost:4000/Dataset</id><content type="html" xml:base="http://localhost:4000/Dataset/">&lt;h2 id=&quot;各类数据库分享连接&quot;&gt;各类数据库分享连接&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://pan.baidu.com/s/1NkOp1GEnQ2ZRYDR9Y_rfbw&quot;&gt;glove&lt;/a&gt;，提取码：yd3j&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pan.baidu.com/s/1q0qGJsIIYS5B8_zwI2z2zg&quot;&gt;nltk&lt;/a&gt;，提取码：82js&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/24709748&quot;&gt;严谨，讲解到位的矩阵求导&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content><author><name>Ricard Huo</name></author><category term="NLP-Dataset" /><summary type="html">各类数据库分享连接 glove，提取码：yd3j nltk，提取码：82js 严谨，讲解到位的矩阵求导</summary></entry><entry><title type="html">CS224n-Assignment4</title><link href="http://localhost:4000/CS224nA4/" rel="alternate" type="text/html" title="CS224n-Assignment4" /><published>2020-08-16T16:04:11+08:00</published><updated>2020-08-16T16:04:11+08:00</updated><id>http://localhost:4000/CS224nA4</id><content type="html" xml:base="http://localhost:4000/CS224nA4/">&lt;h2 id=&quot;奇怪的问题&quot;&gt;奇怪的问题&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;python&amp;lt;3.7的话，会遇到一个奇怪的符号导致后面的注释出现问题，把这个-&amp;gt;删了就行，它的功能类似于argparse。&lt;/li&gt;
  &lt;li&gt;LSTM的输入进行了打包处理，操作不当会遇到奇怪的错误，一定要仔细看注释的说明。&lt;/li&gt;
  &lt;li&gt;decode层在&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;init()&lt;/code&gt;中的定义位置不当，会导致sanity测试出错&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;python sanity_check.py 1f&lt;/code&gt;，虽然forward层是一样的。&lt;/li&gt;
  &lt;li&gt;如果想download代码进行测试，记得下载nltk数据库，这边需要一些微操把数据放在该放的地方。&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;nmt&quot;&gt;NMT&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/yingtaoHuo/CS224n-assignment/tree/master/a4&quot;&gt;(a)-&amp;gt;(f)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;(g)attention经过mask后， (src_len,max_len)之间的0变为-inf， 因为后面有softmax，exp(-无穷)=0，解决exp(0)=1在softmax层占比过大的问题。&lt;/li&gt;
  &lt;li&gt;(j)对于三个attention现在并不是很懂，以后再来填坑，code里用multiplicative attention而不是dot product attention,我一直以为是因为s，h维度不匹配，通过w改变h的维度以便进行bmm操作。以目前所知，dot product attention优势在于计算简单，缺点在于query的size要和h_enc的size必须匹配。additive attention的优势在于可训练f(h_enc,query)可以更好地表达两者间的关系(intution), 缺点在于参数过多(奥卡姆剃刀发出警告)&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;analyzing-nmt-systems&quot;&gt;Analyzing NMT Systems&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;不会西班牙语，等以后有机会再填坑。&lt;/li&gt;
  &lt;li&gt;不想浪费计算资源整这些有的没的，毕竟前人已经证明这套方法的行之有效。&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;模型的训练使用&quot;&gt;模型的训练，使用&lt;/h2&gt;
&lt;h3 id=&quot;模型训练&quot;&gt;模型训练&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;batch_iter里的一个小知识点：yield&lt;/li&gt;
  &lt;li&gt;source，target中的句子添加pad，确保文本对齐&lt;/li&gt;
  &lt;li&gt;定义nn.Embeddings时注意padding_idx的设置&lt;/li&gt;
  &lt;li&gt;BiLSTM对输入语句进行整体编码，获得dec_init（h_projection(h_o)，c_projection(c_o)），h_enc&lt;/li&gt;
  &lt;li&gt;生成enc_masks,具体目的是标记每个句子结束的部分，计算attention时不会考虑到后面无意义部分，参见NMT_3&lt;/li&gt;
  &lt;li&gt;decode部分，就是把与encode输入句子parallel的句子，逐词输入，每个词各种计算得到一个向量(1*h, 因为一个batch一起训练，所以输出是b/**h)，最后再append，stack到一起&lt;/li&gt;
  &lt;li&gt;对于每个词，由(1,h)-&amp;gt;(1,tgt_len)（经由target_vocab_projection），P大小为(b*tgt_len)&lt;/li&gt;
  &lt;li&gt;这里的gather函数，类似于一个挑选的意思，这个需要理解一下。gather((tgt_len,b,h),(tgt_len,b,1),dim=-1),返回的是(tgt_len,b,1),这个操作的目的时，从hidden输出中挑选出有意义的那一位（根据单词的one-hot向量）&lt;/li&gt;
  &lt;li&gt;再根据mask去掉end后面的数值，求和，返回这个batch的总分数
    &lt;h3 id=&quot;使用过程&quot;&gt;使用过程&lt;/h3&gt;
    &lt;p&gt;&lt;a href=&quot;http://web.stanford.edu/class/cs224n/assignments/a4.pdf&quot;&gt;2.b&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;</content><author><name>Ricard Huo</name></author><category term="NLP-Basic" /><summary type="html">奇怪的问题 python&amp;lt;3.7的话，会遇到一个奇怪的符号导致后面的注释出现问题，把这个-&amp;gt;删了就行，它的功能类似于argparse。 LSTM的输入进行了打包处理，操作不当会遇到奇怪的错误，一定要仔细看注释的说明。 decode层在init()中的定义位置不当，会导致sanity测试出错python sanity_check.py 1f，虽然forward层是一样的。 如果想download代码进行测试，记得下载nltk数据库，这边需要一些微操把数据放在该放的地方。</summary></entry><entry><title type="html">CS224n-Assignment3</title><link href="http://localhost:4000/CS224nA3/" rel="alternate" type="text/html" title="CS224n-Assignment3" /><published>2020-08-13T02:14:20+08:00</published><updated>2020-08-13T02:14:20+08:00</updated><id>http://localhost:4000/CS224nA3</id><content type="html" xml:base="http://localhost:4000/CS224nA3/">&lt;h1 id=&quot;assignment3-write-answer&quot;&gt;assignment3 write Answer&lt;/h1&gt;
&lt;h2 id=&quot;machine-learning--neural-networks&quot;&gt;Machine Learning &amp;amp; Neural Networks&lt;/h2&gt;
&lt;h3 id=&quot;a-adam-optimizer&quot;&gt;(a) Adam Optimizer&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;b=0.9决定了，每次参数更新更大的取决于历史遗留梯度，而非本轮计算出的梯度，所以假设这次梯度varing了，乘一个0.1也不会掀起什么大风大浪。m的出现会使每次更新迭代的步长更加稳定，不会忽小忽大，提高了收敛效率。&lt;/li&gt;
  &lt;li&gt;这个没理解，数学直觉告诉我，第一个式子已经起到了这个式子的作用。后面有机会结和实际项目，再来填坑。
    &lt;h3 id=&quot;b-regularization-technique-dropout&quot;&gt;(b) Regularization technique-Dropout&lt;/h3&gt;
  &lt;/li&gt;
  &lt;li&gt;1式带入2式&lt;/li&gt;
  &lt;li&gt;训练期间，dropout可以提高模型的泛化能力。但是在测试期间，dropout会提高模型的不确定性，导致测试结果无法体现模型的performance。&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;dependency-parsing&quot;&gt;Dependency Parsing&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://spellonyou.github.io/2020/06/cs224n-19w-a3/&quot;&gt;图&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;2*n，进n次，出n次&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/yingtaoHuo/CS224n-assignment/tree/master/a3/student&quot;&gt;见代码&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;参考目录：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization&quot;&gt;参数初始化&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;https://spellonyou.github.io/2020/06/cs224n-19w-a3/&lt;/li&gt;
&lt;/ol&gt;</content><author><name>Ricard Huo</name></author><category term="NLP-Basic" /><summary type="html">assignment3 write Answer Machine Learning &amp;amp; Neural Networks (a) Adam Optimizer b=0.9决定了，每次参数更新更大的取决于历史遗留梯度，而非本轮计算出的梯度，所以假设这次梯度varing了，乘一个0.1也不会掀起什么大风大浪。m的出现会使每次更新迭代的步长更加稳定，不会忽小忽大，提高了收敛效率。 这个没理解，数学直觉告诉我，第一个式子已经起到了这个式子的作用。后面有机会结和实际项目，再来填坑。 (b) Regularization technique-Dropout 1式带入2式 训练期间，dropout可以提高模型的泛化能力。但是在测试期间，dropout会提高模型的不确定性，导致测试结果无法体现模型的performance。</summary></entry><entry><title type="html">NLP_2020 Sentiment Analysis</title><link href="http://localhost:4000/PapaerReading/" rel="alternate" type="text/html" title="NLP_2020 Sentiment Analysis" /><published>2020-08-08T03:42:20+08:00</published><updated>2020-08-08T03:42:20+08:00</updated><id>http://localhost:4000/PapaerReading</id><content type="html" xml:base="http://localhost:4000/PapaerReading/">&lt;ol&gt;
  &lt;li&gt;本文分析各个文本预处理过程在情感分析任务中的作用。 *
&lt;em&gt;A Comprehensive Analysis of Preprocessing for Word Representation Learning in Affective Tasks&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;本文贡献了一个中文多模态数据集，创新点在于数据标注的方式（one mutlimodal annotation and three unimodal annotations for each video clip）。并且进行了一系列的实验，发布了该数据集的baseline。*
&lt;em&gt;CH-SIMS: A Chinese Multimodal Sentiment Analysis Dataset with Fine-grained Annotations of Modality&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;本文提出一种新颖的预处理办法：Sentiment Knowledge Enhanced Pre-training，为了解决目前sentiment words和aspect-sentiment pairs在预处理中被忽视的问题。*
&lt;em&gt;SKEP: Sentiment Knowledge Enhanced Pre-training for Sentiment Analysis&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;本文提出cross domain的模型，主要为了解决labeled数据不足的问题。*
&lt;em&gt;Adversarial and Domain-Aware BERT for Cross-Domain Sentiment Analysis&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;本文提出了一套无监督，跨语种，情感分类模型，命名为multi-view encoder-classifier(MVEC)，解决部分语种缺少标注数据和大型语料库的问题。
&lt;em&gt;Cross-Lingual Unsupervised Sentiment Classification with Multi-View Transfer Learning&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;关于Aspect sentiment analysis问题，本文提出了过去的研究者们专注于sentence-level sentiment analysis，忽略了document-level的信息，并举出明显案例佐证观点。以下是对于Aspect sentiment analysis的解释。*
Aspect sentiment analysis: Aspect-based sentiment analysis is a text analysis technique that breaks down text into aspects (attributes or components of a product or service), and then allocates each one a sentiment level (positive, negative or neutral).
&lt;em&gt;Aspect Sentiment Classification with Document-level Sentiment Preference Modeling&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;本文提出此前的aspect情感研究存在一个问题：人们通过attention机制，强调opinion word与aspect的关系，但是因为语言间复杂的关系，模型往往会confuse这些关系。本文提出通过构建有效的语法树来解决这一问题。*
&lt;em&gt;Relational Graph Attention Network for Aspect-based Sentiment Analysis&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;本文认为情感分析，情绪分析的结果对于嘲讽检测有正面意义，提出一个多任务框架，使用环境为多模态的场景。这个研究证明，情感检测得到的结果可以提高嘲讽检测模型的效果。
&lt;em&gt;Sentiment and Emotion help Sarcasm? A Multi-task Learning Framework for Multi-Modal Sarcasm, Sentiment and Emotion Analysis&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;本文首先将ABSA问题，拆解为三个子问题的集合，提出三个子问题间的交互关系尚未被挖掘这一事实，于是提出想通过构建aspect与opinion的关系对三个子问题的协作信号进行编码（这句话过于拗口，简单点就是，加了点先验知识，比如‘美味’一般情况下不会对应‘地板’，大概率会对应‘食物’类别下的名词）。*
&lt;em&gt;Relation-Aware Collaborative Learning for Unified Aspect-Based Sentiment Analysis&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;本文尝试拓展bert模型的使用方法，提高类bert模型在SST数据集上，phrase-level sentiment analysis的成绩。通过构建一棵二元树，总结一段phrase的上下文信息，实验证明在phrase-level sentiment classification获得较好效果。后面还有实验可以证明该模型的迁移效果较好。最后设计了可视化方法去展示这一模型的优势。 *
&lt;em&gt;SentiBERT: A Transferable Transformer-Based Architecture for Compositional Sentiment Semantics&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;本文尝试提高domain adversial模型，在cross domain（combat domain gap between different applications）问题上的表现，通过使用图卷积自动编码器获得domain域的信息来达到目的。 *
&lt;em&gt;KinGDOM: Knowledge-Guided DOMain adaptation for sentiment analysis&lt;/em&gt;&lt;/li&gt;
&lt;/ol&gt;</content><author><name>Ricard Huo</name></author><category term="NLP-Sentiment" /><summary type="html">本文分析各个文本预处理过程在情感分析任务中的作用。 * A Comprehensive Analysis of Preprocessing for Word Representation Learning in Affective Tasks 本文贡献了一个中文多模态数据集，创新点在于数据标注的方式（one mutlimodal annotation and three unimodal annotations for each video clip）。并且进行了一系列的实验，发布了该数据集的baseline。* CH-SIMS: A Chinese Multimodal Sentiment Analysis Dataset with Fine-grained Annotations of Modality 本文提出一种新颖的预处理办法：Sentiment Knowledge Enhanced Pre-training，为了解决目前sentiment words和aspect-sentiment pairs在预处理中被忽视的问题。* SKEP: Sentiment Knowledge Enhanced Pre-training for Sentiment Analysis 本文提出cross domain的模型，主要为了解决labeled数据不足的问题。* Adversarial and Domain-Aware BERT for Cross-Domain Sentiment Analysis 本文提出了一套无监督，跨语种，情感分类模型，命名为multi-view encoder-classifier(MVEC)，解决部分语种缺少标注数据和大型语料库的问题。 Cross-Lingual Unsupervised Sentiment Classification with Multi-View Transfer Learning 关于Aspect sentiment analysis问题，本文提出了过去的研究者们专注于sentence-level sentiment analysis，忽略了document-level的信息，并举出明显案例佐证观点。以下是对于Aspect sentiment analysis的解释。* Aspect sentiment analysis: Aspect-based sentiment analysis is a text analysis technique that breaks down text into aspects (attributes or components of a product or service), and then allocates each one a sentiment level (positive, negative or neutral). Aspect Sentiment Classification with Document-level Sentiment Preference Modeling 本文提出此前的aspect情感研究存在一个问题：人们通过attention机制，强调opinion word与aspect的关系，但是因为语言间复杂的关系，模型往往会confuse这些关系。本文提出通过构建有效的语法树来解决这一问题。* Relational Graph Attention Network for Aspect-based Sentiment Analysis 本文认为情感分析，情绪分析的结果对于嘲讽检测有正面意义，提出一个多任务框架，使用环境为多模态的场景。这个研究证明，情感检测得到的结果可以提高嘲讽检测模型的效果。 Sentiment and Emotion help Sarcasm? A Multi-task Learning Framework for Multi-Modal Sarcasm, Sentiment and Emotion Analysis 本文首先将ABSA问题，拆解为三个子问题的集合，提出三个子问题间的交互关系尚未被挖掘这一事实，于是提出想通过构建aspect与opinion的关系对三个子问题的协作信号进行编码（这句话过于拗口，简单点就是，加了点先验知识，比如‘美味’一般情况下不会对应‘地板’，大概率会对应‘食物’类别下的名词）。* Relation-Aware Collaborative Learning for Unified Aspect-Based Sentiment Analysis 本文尝试拓展bert模型的使用方法，提高类bert模型在SST数据集上，phrase-level sentiment analysis的成绩。通过构建一棵二元树，总结一段phrase的上下文信息，实验证明在phrase-level sentiment classification获得较好效果。后面还有实验可以证明该模型的迁移效果较好。最后设计了可视化方法去展示这一模型的优势。 * SentiBERT: A Transferable Transformer-Based Architecture for Compositional Sentiment Semantics 本文尝试提高domain adversial模型，在cross domain（combat domain gap between different applications）问题上的表现，通过使用图卷积自动编码器获得domain域的信息来达到目的。 * KinGDOM: Knowledge-Guided DOMain adaptation for sentiment analysis</summary></entry><entry><title type="html">CS224n-Assignment2</title><link href="http://localhost:4000/CS224nA2/" rel="alternate" type="text/html" title="CS224n-Assignment2" /><published>2020-08-08T03:42:20+08:00</published><updated>2020-08-08T03:42:20+08:00</updated><id>http://localhost:4000/CS224nA2</id><content type="html" xml:base="http://localhost:4000/CS224nA2/">&lt;ol&gt;
  &lt;li&gt;本来最近在完成cs224n的课程，就记录一下自己在完成过程中较易堵住的地方吧。&lt;/li&gt;
  &lt;li&gt;加载glovce预训练模型，因为在墙内，所以比较麻烦，先从我提供的百度云盘中下载glove.6B.zip，解压到同一目录下，&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gensim.scripts.glove2word2vec()&lt;/code&gt;将其转为word2vec格式的模型，再由&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;load_word2vec_format()&lt;/code&gt;函数进行读取。&lt;/li&gt;
  &lt;li&gt;思考glove模型的bias问题，bias怎么来的，如何解决&lt;/li&gt;
  &lt;li&gt;word2vec的训练迭代过程&lt;/li&gt;
  &lt;li&gt;type创建类与class的区别，&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;isinstance()&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;numpy,dot,broadcast,multiply区别&lt;/li&gt;
  &lt;li&gt;word2vec训练源码：&lt;a href=&quot;https://github.com/yingtaoHuo/CS224n-assignment/blob/master/a2/word2vec.py&quot;&gt;skip-gram模型&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;参考文献&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;https://looperxx.github.io/CS224n-2019-Assignment/&lt;/li&gt;
  &lt;li&gt;http://web.stanford.edu/class/cs224n/&lt;/li&gt;
  &lt;li&gt;http://moverzp.com/2019/05/19/CS-224n-2019-Assignment-2-word2vec-Coding-%E2%80%94%E2%80%94Part-1/&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.runoob.com/numpy/numpy-broadcast.html&quot;&gt;numpy广播机制&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content><author><name>Ricard Huo</name></author><category term="NLP-Basic" /><summary type="html">本来最近在完成cs224n的课程，就记录一下自己在完成过程中较易堵住的地方吧。 加载glovce预训练模型，因为在墙内，所以比较麻烦，先从我提供的百度云盘中下载glove.6B.zip，解压到同一目录下，gensim.scripts.glove2word2vec()将其转为word2vec格式的模型，再由load_word2vec_format()函数进行读取。 思考glove模型的bias问题，bias怎么来的，如何解决 word2vec的训练迭代过程 type创建类与class的区别，isinstance() numpy,dot,broadcast,multiply区别 word2vec训练源码：skip-gram模型</summary></entry><entry><title type="html">ForeverYoung</title><link href="http://localhost:4000/Forever-Young/" rel="alternate" type="text/html" title="ForeverYoung" /><published>2018-03-11T18:32:20+08:00</published><updated>2018-03-11T18:32:20+08:00</updated><id>http://localhost:4000/Forever-Young</id><content type="html" xml:base="http://localhost:4000/Forever-Young/">&lt;p&gt;If you have known what you will face in advance, will you have enough courage to come again?
From World War, to revolution and ultimately rebirth, Forever Young is the story of four generations spanning a hundred years of modern Chinese history. Each generation faces its own unique set of challenges. Up against corporate corruption, the trials and tribulations of the cultural revolution, and one’s duty to nation in time of war, they are faced with choosing their individual paths through history. But as the challenges and turmoil of each generation may differ as time changes, what was learned in the past cannot help but effect the choices that are made in the future as it is passed down from generation to generation. And that is the universal message that being true to yourself is precious. It is the source of strength that empowers one to become the person they want to be, to march forward as far as their hearts desire, into the future.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;font color=&quot;#EE2C2C&quot;&gt;&quot;爱你所爱，行你所行，听从你心，无问西东&quot;
&lt;/font&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/film-foreveryoung-1.jpg&quot; alt=&quot;Yosh Ginsu&quot; /&gt;&lt;/p&gt;</content><author><name>Ricard Huo</name></author><category term="Blog" /><category term="Forever Young" /><summary type="html">If you have known what you will face in advance, will you have enough courage to come again? From World War, to revolution and ultimately rebirth, Forever Young is the story of four generations spanning a hundred years of modern Chinese history. Each generation faces its own unique set of challenges. Up against corporate corruption, the trials and tribulations of the cultural revolution, and one’s duty to nation in time of war, they are faced with choosing their individual paths through history. But as the challenges and turmoil of each generation may differ as time changes, what was learned in the past cannot help but effect the choices that are made in the future as it is passed down from generation to generation. And that is the universal message that being true to yourself is precious. It is the source of strength that empowers one to become the person they want to be, to march forward as far as their hearts desire, into the future.</summary></entry></feed>