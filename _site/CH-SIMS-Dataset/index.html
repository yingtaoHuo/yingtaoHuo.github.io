<!DOCTYPE html>
<html lang="en">
<head>
	<meta charset="utf-8">
	<title>CH-SIMS-Dataset - My personal blog - Richard Huo</title>
	<meta name="description" content="write some words">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
	<meta property="og:image" content="">
	<link rel="shortcut icon" href="/assets/img/favicon/favicon.ico" type="image/x-icon">
	<link rel="apple-touch-icon" href="/assets/img/favicon/apple-touch-icon.png">
	<link rel="apple-touch-icon" sizes="72x72" href="/assets/img/favicon/apple-touch-icon-72x72.png">
	<link rel="apple-touch-icon" sizes="114x114" href="/assets/img/favicon/apple-touch-icon-114x114.png">
	<!-- Chrome, Firefox OS and Opera -->
	<meta name="theme-color" content="#311e3e">
	<!-- Windows Phone -->
	<meta name="msapplication-navbutton-color" content="#311e3e">
	<!-- iOS Safari -->
	<meta name="apple-mobile-web-app-status-bar-style" content="#311e3e">
	<!-- Google Fonts -->
	<link href="https://fonts.googleapis.com/css?family=Montserrat:300,400,700" rel="stylesheet">
	<link href="https://fonts.googleapis.com/css?family=Lato:300,400,700" rel="stylesheet">
	<!-- Font Awesome -->
	<link rel="stylesheet" href="/assets/fonts/font-awesome/css/font-awesome.min.css">
	<!-- Styles -->
	<link rel="stylesheet" href="/assets/css/main.css">
</head>

<body>
  <head>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
  <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
          tex2jax: {
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
          inlineMath: [['$','$']]
          }
      });
  </script>
</head>
<div class="flex-container">
  <header class="main-header">
  <div class="wrapper">
    <div class="header-flex">
      <div class="menu-icon-container">
        <span class="menu-icon"><i class="fa fa-bars" aria-hidden="true"></i></span>
      </div>
      <nav class="main-nav">
        <span class="menu-icon-close"><i class="fa fa-times" aria-hidden="true"></i></span>
        <ul>
          <li><a href="/">Blog</a></li>
          <li><a href="/about">About</a></li>
        </ul>
      </nav>
      <p class="logo"><a href="/">Richard Huo</a></p>
      <div class="search-icon-container">
        <span class="search-icon"><a><i class="fa fa-search" aria-hidden="true"></i></a></span>
      </div>
    </div>
  </div>
</header> <!-- End Header -->

  <article class="article-page">
    <div class="page-image">
      <div class="cover-image" style="background: url(/assets/img/post-2.jpg) center no-repeat; background-size: cover;"></div>
    </div>
    <div class="wrapper">
      <div class="page-content">
        <div class="header-page">
          <h1 class="page-title">CH-SIMS-Dataset</h1>
          <div class="page-date"><time datetime="2021-04-29 15:56:20 +0800">2021, Apr 29</time></div>
        </div>
        <h2 id="motivation">Motivation</h2>
<ol>
  <li>Unified multimodal annotations can not reflect the independent sentiment of single modalities, and limit the model to capture difference between modalities.</li>
  <li>Chinese single- and multi-modal sentiment analysis dataset, which contains 2281 refined video segments/clips in wild with both multimodal and independent unimodal annotations.</li>
  <li>They propose a multi-task learning framework based on late fusion as the baseline.</li>
  <li>Source code provided by author are available(https://github.com/thuiar/MMSA/)</li>
</ol>

<h2 id="related-works">Related Works</h2>

<h3 id="multimodal-datasets">Multimodal Datasets</h3>
<p>IEMOCAP()
YouTube()
MOUD()
ICT-MMMO()
MOSI()
CMU-MOSEI()</p>

<h3 id="multimodal-sentiment-analysis">Multimodal Sentiment Analysis</h3>
<ol>
  <li>General framework proposed(https://arxiv.org/pdf/1707.09538.pdf) which is composed of representation learning on intra-modality and feature concatenation on inter-modality.</li>
  <li>(https://arxiv.org/pdf/1806.06176.pdf)Tasi try to factorize representations into two sets of independent factors: multimodal discriminative and modality-specific generative factors.</li>
</ol>

<h3 id="multi-task-learning">Multi-task Learning</h3>
<ol>
  <li>The aim of multi-task learning is to improve the generalization performance of multiple related tasks by utlizing useful information contained in these tasks. The classical framework is that different tasks share the first several layers and then have task-specific parameters in the subsequent layers. In this task, multimodal multi-task learning framework is applied for the verification and feasibility of independent unimodal annotations.</li>
</ol>

<h4 id="problems">Problems</h4>
<p>Negative segments are more than positive segments.</p>

<h2 id="extracted-features">Extracted Features</h2>
<h4 id="text">Text</h4>
<ol>
  <li>add two unique tokens to indicate the beginning and the end for each transcript.</li>
  <li>pre-trained Chinese BERTbase word embeddings are used to obtain word vectors from transcripts. It is worth noting that they do not use word segmentation tools due to the characteristic of BERT.</li>
</ol>

<h4 id="audio">Audio</h4>
<ol>
  <li>use LibROSA speech toolkit with default parameters to extract acoustic features at 22050Hz.</li>
  <li>Totally, 33-dimensional frame-level acoustic features are extracted, including 1-dimensional logarithmic fundamental frequency (log F0), 20-dimensional Melfrequency cepstral coefficients (MFCCs) and 12-dimensional Constant-Q chromatogram (CQT).These features are related to emotions and tone of speech according to (<a href="https://hcsi.cs.tsinghua.edu.cn/Paper/Paper18/MM-LIRUNNAN.pdf">Li et al., 2018</a>).</li>
</ol>

<h4 id="vision">Vision</h4>
<ol>
  <li>They employ the MTCNN face detection algorithm (<a href="https://arxiv.org/ftp/arxiv/papers/1604/1604.02878.pdf">Zhang et al., 2016a</a>) to extract aligned faces.</li>
  <li>Then they use MultiComp Openface toolkit to extract the set of 68 facial landmarks, 17 facial action units, head pose, head orientation, and eye gaze. Lastly, 709-dimensional frame-level visual features are extracted in total.</li>
</ol>

<h4 id="for-intra-modal-representation">For Intra-Modal Representation</h4>
<ol>
  <li>For the convenience in text, audio and vision, we assume that:
\(L^{u}：Sequence Length\)
\(D_{i}^{u}：Initial Feature\)
\(D_{r}^{u}：RepresentationFeatures\)
    <blockquote>
      <p>where u ∈ {t, a, v}, represent the sequence length,
initial feature is extracted by section 3.3
and representation feature learned by unimodal
feature extractor, respectively. The batch size is B.</p>
    </blockquote>
  </li>
  <li>计算公式
\(R_{u}=S_{u}\left(I_{u}\right)\)</li>
  <li>They use a Long Short-Term Memory (LSTM)
network, a deep neural network with three hidden
layers of weights Wa and a deep neural network
with three hidden layers of weights Wv to extract
textual, acoustic and visual embeddings, respectively.</li>
</ol>

<h4 id="for-inter-modal-representation">For Inter-Modal Representation</h4>
<p>They try three fusion methods: LF-DNN, TFN and LMF</p>

<h4 id="multimodal-multitask-learning-framework">Multimodal Multitask Learning Framework</h4>
<ol>
  <li>Objective Function
\(\min \frac{1}{N_{t}} \sum_{n=1}^{N_{t}} \sum_{i} \alpha_{i} L\left(y_{i}^{n}, \hat{y}_{i}^{n}\right)+\sum_{j} \beta_{j}\left\|W_{j}\right\|_{2}^{2}\)
    <blockquote>
      <p>Lastly, we use a three-layer DNN to generate
outputs of different tasks. In this work, we treat
these tasks as regression models.</p>
    </blockquote>
  </li>
</ol>

        <div class="page-footer">
          <div class="page-tag">
            <span>Tags:</span>
            
            <a href="/tags#MultiModal" class="tag">| MultiModal</a>
            
          </div><!-- End Tags -->
          <div class="page-share">
            <span>Share:</span>
            <a href="https://twitter.com/intent/tweet?text=CH-SIMS-Dataset&url=http://localhost:4000/CH-SIMS-Dataset/" title="Share on Twitter" rel="nofollow" target="_blank"><i class="fa fa-twitter" aria-hidden="true"></i></a>
            <a href="https://facebook.com/sharer.php?u=http://localhost:4000/CH-SIMS-Dataset/" title="Share on Facebook" rel="nofollow" target="_blank"><i class="fa fa-facebook" aria-hidden="true"></i></a>
            <a href="https://plus.google.com/share?url=http://localhost:4000/CH-SIMS-Dataset/" title="Share on Google+" rel="nofollow" target="_blank"><i class="fa fa-google" aria-hidden="true"></i></a>
          </div><!-- End Share -->
        </div>
        <section class="author-box">
  <img src="/assets/img/huo-face.jpg" alt="Richard Huo" class="author-img">
  <div class="author-desc">
    <h2>Richard Huo</h2>
    <p>Time will tell, 热爱生活的当代码农，狼人杀新手玩家。</p>
    <ul>
      
        <li class="email"><a href="mailto:510344837@qq.com"><i class="fa fa-envelope-o"></i></a></li>
      
      
        <li class="phone"><a href="tel:044 825 5523"><i class="fa fa-phone" aria-hidden="true"></i></a></li>
      
      
        <li class="website"><a href="http://yingtaohuo.github.io" target="_blank"><i class="fa fa-globe"></i></a></li>
      
      
        <li class="twitter"><a href="https://twitter.com/artemsheludko_" target="_blank"><i class="fa fa-twitter" aria-hidden="true"></i></a></li>
      
    </ul>
  </div>
</section>

        <div class="recent-box">
  <h2 class="recent-title">Recent post</h2>
  <div class="recent-list">
    
      
        <a href="/CH-SIMS-Dataset/" class="recent-item" style="background: url(/assets/img/post-2.jpg) center no-repeat; background-size: cover;"><span>CH-SIMS-Dataset</span></a>
      
    
      
        <a href="/ACSA/" class="recent-item" style="background: url(/assets/img/post-6.jpg) center no-repeat; background-size: cover;"><span>Aspect-based Sentiment Analysis</span></a>
      
    
      
        <a href="/Attention-is-all-you-need/" class="recent-item" style="background: url(/assets/img/post-2.jpg) center no-repeat; background-size: cover;"><span>Attention is all you need</span></a>
      
    
      
        <a href="/TheEightHundred/" class="recent-item" style="background: url(/assets/img/xie_jinyuan.jpg) center no-repeat; background-size: cover;"><span>八佰观后感</span></a>
      
    
  </div>
</div> <!-- End Recent-Box -->
        <div class="newsletter" id="mc_embed_signup">
  <h2 class="newsletter-title">Newsletter</h2>
  <div class="form-container">
    <p>Subscribe here to get our latest updates</p>
    <form action="//" method="post" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate>
      <label class="screen-reader-text" for="mce-EMAIL">Email Address</label>
      <div class="newsletter-box" id="mc_embed_signup_scroll">
        <input type="email" name="EMAIL" placeholder="Email address" class="email-input" id="mce-EMAIL" required>
        <input type="submit" value="Subscribe" name="subscribe" class="subscribe-btn" id="mc-embedded-subscribe">
      </div>
    </form>
  </div>
</div> <!-- End Newsletter -->

        <section class="comment-area">
  <div class="comment-wrapper">
    
  </div>
</section> <!-- End Comment Area -->

      </div>
    </div> <!-- End Wrapper -->
  </article>
  <div class="search-box">
  <div class="wrapper">
    <div class="search-grid">
      <form class="search-form">
        <div id="search-container">
          <input type="text" id="search-input" class="search" placeholder="Search">
        </div>
      </form>
      <ul id="results-container" class="results-search"></ul>
      <div class="icon-close-container">
        <span class="search-icon-close"><i class="fa fa-times" aria-hidden="true"></i></span>
      </div>
    </div>
  </div>
</div>

  <footer class="main-footer">
  <div class="copyright">
    <p>2021 &copy; Richard Huo</p>
  </div>
</footer> <!-- End Footer -->

</div>

  <!-- JS -->
<script src="/assets/js/jquery-3.2.1.min.js"></script>
<script src="/assets/js/jekyll-search.js"></script>
<script>
  SimpleJekyllSearch({
    searchInput: document.getElementById('search-input'),
    resultsContainer: document.getElementById('results-container'),
    json: '/search.json',
    searchResultTemplate: '<li><a href="{url}" title="{desc}">{title}</a></li>',
    noResultsText: 'No results found',
    fuzzy: false,
    exclude: ['Welcome']
  });
</script>
<script src="/assets/js/main.js"></script>

  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', '', 'auto');
  ga('send', 'pageview');
</script> <!-- End Analytics -->

</body>
</html>
